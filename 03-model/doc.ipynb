{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Documentación__\n",
    "\n",
    "Algo similar a la documentación de un paquete, que servirá para organizar mejor los métodos empleados y su implementación, especialmente las funciones a usar y la implementación numérica cuando sea necesario.\n",
    "\n",
    "## __Método__\n",
    "\n",
    "El modelo implementado se basa en el algoritmo ___gradient descent___, con la función de costo $\\chi^2$, definida como\n",
    "\n",
    "$$\\chi^2 = \\frac{1}{N}\\sum_{i=1}^{N}(Y_i - \\hat{Y}_i)$$\n",
    "\n",
    "con $N$ el número de datos, para optimizar el modelo multilineal\n",
    "\n",
    "$$Y(x^{(1)},\\dotsc,x^{(M)};w_1,\\dotsc,w_M,b) = \\sum_{j=1}^{M} \\{w_j x^{(j)}\\} + b = w_1 x^{(1)} + w_2 x^{(2)} + \\dotsc + w_M x^{(M)} + b$$\n",
    "\n",
    "con $M+1$ parámetros dependiente de $M$ variables, a partir de un conjunto de datos experimentales. En la expresión para $\\chi^2$, $Y_i$ y $\\hat{Y}_i$ corresponden, respectivamente, a los valores predichos por el modelo en cada época de entrenamiento, y a los valores esperados experimentalmente.\n",
    "\n",
    "El método gradient descent utiliza la función de costo ($\\chi^2$ en nuestro caso) para encontrar raíces de la ecuación dada por la derivada de la función de costo, se trata de una aproximación numérica a problema de optimización. El algoritmo está descrito entonces por la relación recursiva:\n",
    "\n",
    "$$w_j = w_j + \\alpha\\frac{\\partial (\\chi^2)}{\\partial w_j}$$\n",
    "$$b = b + \\beta\\frac{\\partial (\\chi^2)}{\\partial b}$$\n",
    "\n",
    "Para cada parámetro de la ecuación. En esta definición, $\\alpha$ y $\\beta$ corresponden a los factores de aprendizaje del modelo, y se usan, principalmente, para evitar la caída en loops infinitos durante la ejecución. Estos parámetros pueden ser en general distintos (incluso para cada $w_j$, definiendo un $\\alpha_j$ en cada caso), pero usualmente se suele usar un solo parámetro global $\\alpha = \\beta$.\n",
    "\n",
    "Para resolver el problema de optimización, se usan diferencias finitas, específicamente el método de derivada central __central derivative__; esto es\n",
    "\n",
    "$$\\frac{\\partial (\\chi^2)}{\\partial w_j} = \\frac{\\chi^2(w_j + \\Delta w_j) - \\chi^2(w_j - \\Delta w_j)}{2\\Delta w_j}$$\n",
    "\n",
    "El método descrito hasta entonces, se repetirá en un loop hasta que el modelo converja. Aquí, definiremos la diferencia del valor de $\\chi^2$ para dos épocas (iteraciones) de entrenamiento distintas, y detendremos el modelo cuando esta relación sea menor que un determinado valor para esta diferencia $\\chi^2_{i+1} - \\chi^2_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __métodos y atributos de la clase ```MultilinearRegression```__\n",
    "\n",
    "#### ```MultilinearRegression.history()```\n",
    "\n",
    "Este método almacenará la información de la función de costo en cada época de entrenamiento. En nuestro caso, history almacena el valor de $\\chi^2$.\n",
    "\n",
    "#### ```MultilinearRegression.score()```\n",
    "\n",
    "Devuelve el valor de coeficiente de determinación $R^2$ definido mediante\n",
    "$$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$$\n",
    "\n",
    "donde\n",
    "$$SS_{res} = \\sum_{i = 1}^{N} (Y_i - \\hat{Y}_i)^2$$\n",
    "$$SS_{res} = \\sum_{i = 1}^{N} (Y_i - \\overline{Y}_i)^2$$\n",
    "\n",
    "con $Y_i$ y $\\hat{Y_i}$ como se definieron anteriormente, y $\\overline{Y}_i$ el valor medio del conjunto de datos observados"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
