{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Documentación__\n",
    "\n",
    "Algo similar a la documentación de un paquete, que servirá para organizar mejor los métodos empleados y su implementación, especialmente las funciones a usar y la implementación numérica cuando sea necesario.\n",
    "\n",
    "## __Método__\n",
    "\n",
    "El modelo implementado se basa en el algoritmo ___gradient descent___, con la función de costo $\\chi^2$, definida como\n",
    "\n",
    "$$\\chi^2 = \\frac{1}{N}\\sum_{i=1}^{N}(Y_i - \\hat{Y}_i)$$\n",
    "\n",
    "con $N$ el número de datos, para optimizar el modelo multilineal\n",
    "\n",
    "$$Y(x^{(1)},\\dotsc,x^{(M)};w_1,\\dotsc,w_M,b) = \\sum_{j=1}^{M} \\{w_j x^{(j)}\\} + b = w_1 x^{(1)} + w_2 x^{(2)} + \\dotsc + w_M x^{(M)} + b$$\n",
    "\n",
    "con $M+1$ parámetros dependiente de $M$ variables, a partir de un conjunto de datos experimentales. En la expresión para $\\chi^2$, $Y_i$ y $\\hat{Y}_i$ corresponden, respectivamente, a los valores predichos por el modelo en cada época de entrenamiento, y a los valores esperados experimentalmente.\n",
    "\n",
    "El método gradient descent utiliza la función de costo ($\\chi^2$ en nuestro caso) para encontrar raíces de la ecuación dada por la derivada de la función de costo, se trata de una aproximación numérica a problema de optimización. El algoritmo está descrito entonces por la relación recursiva:\n",
    "\n",
    "$$w_j = w_j + \\alpha\\frac{\\partial (\\chi^2)}{\\partial w_j}$$\n",
    "$$b = b + \\beta\\frac{\\partial (\\chi^2)}{\\partial b}$$\n",
    "\n",
    "Para cada parámetro de la ecuación. En esta definición, $\\alpha$ y $\\beta$ corresponden a los factores de aprendizaje del modelo, y se usan, principalmente, para evitar la caída en loops infinitos durante la ejecución. Estos parámetros pueden ser en general distintos (incluso para cada $w_j$, definiendo un $\\alpha_j$ en cada caso), pero usualmente se suele usar un solo parámetro global $\\alpha = \\beta$.\n",
    "\n",
    "Para resolver el problema de optimización, se usan diferencias finitas, específicamente el método de derivada central __central derivative__; esto es\n",
    "\n",
    "$$\\frac{\\partial (\\chi^2)}{\\partial w_j} = \\frac{\\chi^2(w_j + \\Delta w_j) - \\chi^2(w_j - \\Delta w_j)}{2\\Delta w_j}$$\n",
    "\n",
    "El método descrito hasta entonces, se repetirá en un loop hasta que el modelo converja. Aquí, definiremos la diferencia del valor de $\\chi^2$ para dos épocas (iteraciones) de entrenamiento distintas, y detendremos el modelo cuando esta relación sea menor que un determinado valor para esta diferencia $\\chi^2_{i+1} - \\chi^2_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __métodos y atributos de la clase ```MultilinearRegression```__\n",
    "\n",
    "##### __```MultilinearRegression.history```__\n",
    "\n",
    "Atributo. Almacenará la información de la función de costo en cada época de entrenamiento. En nuestro caso, history almacena el valor de $\\chi^2$.\n",
    "\n",
    "##### __```MultilinearRegression.W, MultilinearRegression.b```__\n",
    "\n",
    "Atributos. Almacenan la información de las caraterísticas $w_j$ del modelo ((M,1) array) y del intercepto $b$\n",
    "\n",
    "##### __```MultilinearRegression.predict(X, w = None, b = None)```__\n",
    "\n",
    "Método. Computa $Y = \\sum_{j=1}^{M} \\{w_j x^{(j)}\\} + b$ para definir el valor predicho por el modelo, dados __```X```__ de forma (N,M), __```w```__ de forma (M,1) y ```b``` un valor numérico.\n",
    "\n",
    "##### __```MultilinearRegression.loss_function(self, X, Y, w = None, b = None)```__\n",
    "\n",
    "Método. Recibe dos arreglos __```X```__ de forma (N,M), e __```Y```__ de forma (N,1) para determinar el valor de la función de costo. En nuestro caso, aquí definimos $\\chi^2$. Se puede pasar nuevamente __```w```__ y __```b```__ como argumentos.\n",
    "\n",
    "##### __```MultilinearRegression.grad(self, X, Y, w, b, Dw = 1e-8, Db = 1e-8)```__\n",
    "\n",
    "Método. Implementa el gradiente numérico definido anteriormente con el método de derivada central. Recibe __```Dw```__ y __```Db```__ como parámetros opcionales para fijar la resolución con la que será calculada la derivada numérica.\n",
    "\n",
    "##### __```MultilinearRegression.optimizer(self, X, Y, w, b, tol, max_epoch, alpha, beta)```__\n",
    "Método. Lleva a cabo el proceso de optimización aplicando el método de gradient descendent como se definió anteriormente. Recibe como argumentos la tolerancia __```tol```__, el número máximo de iteraciones que ejecutará el modelo __```max_epoch```__, y los parámetros de aprendizaje __```alpha```__ y __```beta```__; estos argumentos son recibidos al llamar el método __```.fit```__. Devuelve los parámetros __```w```__ y __```b```__ optimizados, así como el número de épocas ejecutadas.\n",
    "\n",
    "##### __```MultilinearRegression.fit(self, X, Y, w0 = None, b0 = None, tol = 1e-13, max_epoch = 500000, alpha = 0.001, beta = 0.001)```__\n",
    "\n",
    "Método. Integra todos los métodos necesarios para realizar el ajuste automáticamente. Puede recibir parámetros de estimación inicial __```w0```__ y __```b0```__ (útiles cuando se conoce más de la naturaleza del modelo, para mejorar la convergencia del algoritmo), la tolerancia __```tol```__ (default 1e-13), el máximo de épicas -iteraciones- __```max_epoch```__ (default 500000), y los parámetros de aprendizaje __```alpha```__ (defalut 0.001) y __```beta```__ (defalut 0.001). Como salida, imprime los valores inicial y final de la función de costo implementada en el modelo.\n",
    "\n",
    "##### __```MultilinearRegression.score```__\n",
    "\n",
    "Método. Devuelve el valor de coeficiente de determinación $R^2$ definido mediante\n",
    "$$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$$\n",
    "\n",
    "donde\n",
    "$$SS_{res} = \\sum_{i = 1}^{N} (Y_i - \\hat{Y}_i)^2$$\n",
    "$$SS_{res} = \\sum_{i = 1}^{N} (Y_i - \\overline{Y}_i)^2$$\n",
    "\n",
    "con $Y_i$ y $\\hat{Y_i}$ como se definieron anteriormente, y $\\overline{Y}_i$ el valor medio del conjunto de datos observados"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
